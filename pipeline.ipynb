{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69d83426",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-11-16T22:04:35.834183Z",
     "iopub.status.busy": "2021-11-16T22:04:35.832620Z",
     "iopub.status.idle": "2021-11-16T22:04:36.830474Z",
     "shell.execute_reply": "2021-11-16T22:04:36.829437Z",
     "shell.execute_reply.started": "2021-11-16T21:50:14.131852Z"
    },
    "papermill": {
     "duration": 1.013483,
     "end_time": "2021-11-16T22:04:36.830704",
     "exception": false,
     "start_time": "2021-11-16T22:04:35.817221",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85c4720",
   "metadata": {
    "papermill": {
     "duration": 0.008447,
     "end_time": "2021-11-16T22:04:36.848683",
     "exception": false,
     "start_time": "2021-11-16T22:04:36.840236",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Predict Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6c7b7c",
   "metadata": {
    "papermill": {
     "duration": 0.008081,
     "end_time": "2021-11-16T22:04:36.865062",
     "exception": false,
     "start_time": "2021-11-16T22:04:36.856981",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Fact-Feeling Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ce61dbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T22:04:36.912144Z",
     "iopub.status.busy": "2021-11-16T22:04:36.911114Z",
     "iopub.status.idle": "2021-11-16T22:05:28.558714Z",
     "shell.execute_reply": "2021-11-16T22:05:28.557717Z",
     "shell.execute_reply.started": "2021-11-16T21:50:15.118901Z"
    },
    "papermill": {
     "duration": 51.685533,
     "end_time": "2021-11-16T22:05:28.558875",
     "exception": false,
     "start_time": "2021-11-16T22:04:36.873342",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /usr/share/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-16 22:04:58.257617: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-16 22:04:58.376731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-16 22:04:58.377440: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-16 22:04:58.378723: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-11-16 22:04:58.380004: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-16 22:04:58.380926: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-16 22:04:58.381796: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-16 22:05:00.337300: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-16 22:05:00.338124: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-16 22:05:00.338791: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-16 22:05:00.339364: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15403 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import pickle\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, initializers, regularizers, constraints, optimizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Conv1D, Embedding, Dropout, GlobalMaxPool1D, SpatialDropout1D, BatchNormalization, Bidirectional, LSTM, GlobalMaxPooling1D, MaxPooling1D, Flatten\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "import tokenization\n",
    "\n",
    "def bert_encode(quotes, responses, tokenizer, max_len=160):\n",
    "    all_tokens = []\n",
    "    all_masks = []\n",
    "    all_segments = []\n",
    "    \n",
    "    for i in range(len(quotes)):\n",
    "        quote = tokenizer.tokenize(quotes[i])\n",
    "        response = tokenizer.tokenize(responses[i])\n",
    "            \n",
    "        quote = quote[:75]\n",
    "        response = response[:75]\n",
    "        \n",
    "        input_sequence = [\"[CLS]\"] + quote + [\"[SEP]\"] + response\n",
    "        pad_len = max_len - len(input_sequence)\n",
    "        \n",
    "        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
    "        tokens += [0] * pad_len\n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "        segment_ids = [0] * max_len\n",
    "        \n",
    "        all_tokens.append(tokens)\n",
    "        all_masks.append(pad_masks)\n",
    "        all_segments.append(segment_ids)\n",
    "    \n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n",
    "\n",
    "\n",
    "def build_model(bert_layer, max_len=512):\n",
    "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
    "\n",
    "    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "    clf_output = sequence_output[:, 0, :]\n",
    "    \n",
    "    out = Dense(1, activation='sigmoid')(clf_output)\n",
    "\n",
    "    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
    "    model.compile(Adam(learning_rate=5.95e-6), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "    \n",
    "def lemmatize_sentence(sentence):\n",
    "\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    \n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(lemmatized_sentence)\n",
    "\n",
    "def sentence_pos_tag(sentence):\n",
    "    text = word_tokenize(sentence)\n",
    "    pos_tag = nltk.pos_tag(text)\n",
    "    pos_tag_res = ''\n",
    "    for i in range(len(pos_tag)):\n",
    "        pos_tag_res += pos_tag[i][1]\n",
    "        pos_tag_res += ' ' if i != len(sentence)-1 else '' \n",
    "    return pos_tag_res\n",
    "\n",
    "def preprocessing(text):\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    text = re.sub(r\" \\d+ \", \" \", text)\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    text = re.sub(r\"[^a-z ]\", \"\", text)\n",
    "    text = re.sub(r\"  \", \" \", text)\n",
    "    text = lemmatize_sentence(text)\n",
    "    return text\n",
    "\n",
    "def label_encoding(label):\n",
    "    if(label == 'fact-based'):\n",
    "        return 0\n",
    "    elif(label == 'feeling-based'):\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "def fact_label_encoding(label):\n",
    "    if(label == 'feeling-based'):\n",
    "        return 0\n",
    "    elif(label == 'fact-based'):\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "def agreement_label_encoding(label):\n",
    "    if(label == 'disagreement'):\n",
    "        return 0\n",
    "    elif(label == 'agreement'):\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "def agreement_label_decoding(label):\n",
    "    if label == 0:\n",
    "        return 'disagreement'\n",
    "    elif label == 1:\n",
    "        return 'agreement'\n",
    "    else:\n",
    "        return 'unsure'\n",
    "\n",
    "module_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\n",
    "bert_layer = hub.KerasLayer(module_url, trainable=True)\n",
    "\n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n",
    "\n",
    "model_BERT = build_model(bert_layer, max_len=160)\n",
    "model_BERT.load_weights('../input/bert-result/model_BERT.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f1ddcef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T22:05:28.589701Z",
     "iopub.status.busy": "2021-11-16T22:05:28.588878Z",
     "iopub.status.idle": "2021-11-16T22:05:28.591549Z",
     "shell.execute_reply": "2021-11-16T22:05:28.591086Z",
     "shell.execute_reply.started": "2021-11-16T21:55:13.457952Z"
    },
    "papermill": {
     "duration": 0.02066,
     "end_time": "2021-11-16T22:05:28.591673",
     "exception": false,
     "start_time": "2021-11-16T22:05:28.571013",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def decode_prediction(pred):\n",
    "    for i in range(len(pred)):\n",
    "        pred[i][0] = 'fact-based' if pred[i][0] == 0 else 'feeling-based'\n",
    "    return np.array(pred)\n",
    "\n",
    "def emotion_predict(quote, responses):\n",
    "    quotes = []\n",
    "    responses_tag = []\n",
    "    \n",
    "    quote = preprocessing(quote)\n",
    "    \n",
    "    for i in range(len(responses)):\n",
    "        quotes.append(quote)\n",
    "\n",
    "        responses[i] = preprocessing(responses[i])\n",
    "        responses_tag.append(sentence_pos_tag(responses[i]))\n",
    "\n",
    "    inputs = bert_encode(responses, responses_tag, tokenizer, max_len=160)\n",
    "\n",
    "    return model_BERT.predict(inputs).round().astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09b6c2ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T22:05:28.616523Z",
     "iopub.status.busy": "2021-11-16T22:05:28.615918Z",
     "iopub.status.idle": "2021-11-16T22:05:34.113322Z",
     "shell.execute_reply": "2021-11-16T22:05:34.113869Z",
     "shell.execute_reply.started": "2021-11-16T22:02:35.462464Z"
    },
    "papermill": {
     "duration": 5.51255,
     "end_time": "2021-11-16T22:05:34.114031",
     "exception": false,
     "start_time": "2021-11-16T22:05:28.601481",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-16 22:05:30.616352: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([['feeling-based'],\n",
       "       ['fact-based'],\n",
       "       ['feeling-based']], dtype='<U13')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quote = 'self confidence is very needed for every one'\n",
    "responses = ['every one have their choice', \n",
    "             'according to book written by mister, there is one example with clearly explanation', \n",
    "             'no, i very bad in confidence']\n",
    "\n",
    "decode_prediction(emotion_predict(quote, responses).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa151a2",
   "metadata": {
    "papermill": {
     "duration": 0.010729,
     "end_time": "2021-11-16T22:05:34.135537",
     "exception": false,
     "start_time": "2021-11-16T22:05:34.124808",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Agree Disagree Classification (Pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dd3abb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T22:05:34.163806Z",
     "iopub.status.busy": "2021-11-16T22:05:34.163231Z",
     "iopub.status.idle": "2021-11-16T22:05:34.167029Z",
     "shell.execute_reply": "2021-11-16T22:05:34.166588Z",
     "shell.execute_reply.started": "2021-11-16T22:03:15.642018Z"
    },
    "papermill": {
     "duration": 0.021533,
     "end_time": "2021-11-16T22:05:34.167131",
     "exception": false,
     "start_time": "2021-11-16T22:05:34.145598",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def agree_predict(quote, response):\n",
    "    emotion = decode_prediction(emotion_predict(quote, [response]).tolist())[0][0]\n",
    "    print(emotion)\n",
    "    model = keras.models.load_model('../input/agreedisagree/double_biGRU_with_emotion_model.h5')\n",
    "    tokenizer_file = open('../input/agreedisagree/tokenizer.pkl', \"rb\")\n",
    "    tokenizer = pickle.load(tokenizer_file)\n",
    "    tokenizer_file.close()\n",
    "    q = tokenizer.texts_to_sequences(np.array([preprocessing(quote)]))\n",
    "    r = tokenizer.texts_to_sequences(np.array([preprocessing(response)]))\n",
    "    e = fact_label_encoding(emotion)\n",
    "    q = sequence.pad_sequences(q, maxlen=1000)\n",
    "    r = sequence.pad_sequences(r, maxlen=1000)\n",
    "    e = np.array([e])\n",
    "    p = model.predict({'quote': q, 'response':r, 'emotion':e})\n",
    "    return agreement_label_decoding(np.argmax(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7418e83",
   "metadata": {
    "papermill": {
     "duration": 0.009358,
     "end_time": "2021-11-16T22:05:34.186164",
     "exception": false,
     "start_time": "2021-11-16T22:05:34.176806",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff38267f",
   "metadata": {
    "papermill": {
     "duration": 0.009559,
     "end_time": "2021-11-16T22:05:34.205569",
     "exception": false,
     "start_time": "2021-11-16T22:05:34.196010",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Fact-Feeling Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "255dd400",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T22:05:34.229738Z",
     "iopub.status.busy": "2021-11-16T22:05:34.229110Z",
     "iopub.status.idle": "2021-11-16T22:05:34.425191Z",
     "shell.execute_reply": "2021-11-16T22:05:34.424753Z",
     "shell.execute_reply.started": "2021-11-16T21:58:10.214178Z"
    },
    "papermill": {
     "duration": 0.210362,
     "end_time": "2021-11-16T22:05:34.425310",
     "exception": false,
     "start_time": "2021-11-16T22:05:34.214948",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['feeling-based'],\n",
       "       ['fact-based'],\n",
       "       ['feeling-based']], dtype='<U13')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quote = 'self confidence is very needed for every one'\n",
    "responses = ['every one have their choice', \n",
    "             'according to book written by mister, there is one example with clearly explanation', \n",
    "             'no, i very bad in confidence']\n",
    "\n",
    "decode_prediction(emotion_predict(quote, responses).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ea6216",
   "metadata": {
    "papermill": {
     "duration": 0.009614,
     "end_time": "2021-11-16T22:05:34.444852",
     "exception": false,
     "start_time": "2021-11-16T22:05:34.435238",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Agreement Disagreement Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "558ef1d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T22:05:34.469490Z",
     "iopub.status.busy": "2021-11-16T22:05:34.468745Z",
     "iopub.status.idle": "2021-11-16T22:05:38.717417Z",
     "shell.execute_reply": "2021-11-16T22:05:38.717903Z",
     "shell.execute_reply.started": "2021-11-16T22:04:09.410144Z"
    },
    "papermill": {
     "duration": 4.262938,
     "end_time": "2021-11-16T22:05:38.718061",
     "exception": false,
     "start_time": "2021-11-16T22:05:34.455123",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feeling-based\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-16 22:05:38.471958: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8005\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'agreement'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quote = \"I am against suicide because you are basically not only harming yourself, but everyone else around you. Let’s not mention it is a cowards way out. I also have a religious but I CAN explain that reason.\"\n",
    "response = \"So true man people only harm the people they love by dying. I am not religious and religiously and non-religiously suicide is wrong.\"\n",
    "\n",
    "agree_predict(quote, response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 73.347548,
   "end_time": "2021-11-16T22:05:41.821248",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-11-16T22:04:28.473700",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
